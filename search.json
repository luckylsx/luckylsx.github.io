[{"title":"在线小工具","url":"/2021/05/29/在线小工具/","content":"\n本文记录了编程人员常用的工具集合，方便自己编程使用，也作为小工具箱方便大家查找。\n\n### 工具转换\n\n* [时间戳](https://tool.lu/timestamp/)\n* [很全的在线工具集合](https://tool.lu/)\n\n<!-- more -->\n\n* [markdown在线编辑器](https://tool.lu/markdown/)\n* [json格式化](https://www.json.cn/)\n* [json 转 struct](https://oktools.net/json2go)\n* [base64 转图片](https://tool.jisuapi.com/base642pic.html)\n* [正则表达式生成](http://tool.chinaz.com/tools/regexgenerate)\n* [图片压缩](https://tinypng.com/)\n\n### 查询类\n\n* [IP 地址查询](https://www.ipaddress.com/)\n* [内容差异比较](https://www.diffchecker.com/diff)\n* [网站证书信息查询](http://web.chacuo.net/netsslctcheck)\n* [聚查网](www.jucha.com)\n\n### 图标\n\n* [阿里巴巴矢量图标库](https://www.iconfont.cn/)\n\n### 在线作图\n\n* [ProcessOn](https://www.processon.com/;jsessionid=63AD3093802D99FD12D81FBD3881824F.jvm1)\n\n### 其他\n\n* [jetbrains激活码](http://idea.medeming.com/jet/)\n* [nginx 证书生成](https://certbot.eff.org/lets-encrypt/centosrhel7-nginx)\n* [中文智能转换变量](https://unbug.github.io/codelf/)\n* [微信公众号排版编辑](https://editor.mdnice.com/)\n* [网站代理](http://www.ddung.org/daili/)\n* [mac 软件下载](https://xclient.info/)\n* [开源操作系统最新信息](https://distrowatch.com/)","tags":["tool"],"categories":["tool"]},{"title":"http 准则","url":"/2021/05/27/http准则/","content":"\n## HTTP 准则\n\n本文档介绍了 Google API 如何与不同的 HTTP 版本和实现结合使用。\n\n## 使用传输协议 (HTTP/*)\n\n<!-- more -->\n\n本部分介绍 Cloud API 可用于在客户端和服务器之间进行通信的支持的传输协议（通常是某个版本的 HTTP），以及我们为您建议的协议使用方式。我们将在下一部分详细介绍请求和响应的结构。\n\n### HTTP 语义\n\n开发 API 客户端代码时，请遵循标准 [HTTP 协议语义](https://datatracker.ietf.org/doc/html/rfc7231)。服务器端代理或 API 堆栈可能仅支持标准 HTTP 功能的子集，并且还可能支持其向后兼容的版本。\n\n需要由 API 的服务器端实现处理的 HTTP 协议语义由服务器堆栈控制。仅当这些功能明确记录为 API 规范（例如缓存支持）的一部分时，才能依赖此类语义。\n\n### HTTP 版本\n\n客户端可以使用客户端平台或其客户端网络允许或者与服务器端代理协商的任何 HTTP/* 协议。支持的协议包括 HTTP/1.0、HTTP/1.1、SPDY/*、HTTP/2 和 QUIC。\n\n某些 API 功能可能只有较新版本的 HTTP 协议（例如服务器推送和优先级）才支持；部分功能仅使用 HTTP/2（例如全双工流式传输）完全指定。如果您需要将这些功能中的任何功能作为 API 规范的一部分，请注意不同 HTTP 版本的限制。\n\n通常，为了提高性能和网络故障的恢复能力，建议使用 HTTP/2。\n\n### 渠道\n\n信道是指 L4 网络连接（TCP 或 UDP 套接字）。客户端应用不应该针对在运行时中如何使用信道来提供 HTTP 请求作出任何假设。在几乎所有情况下，信道都是由代表服务器进程的代理来终止。\n\n对于 HTTP/1.1 客户端，应始终重复使用 TCP 连接 (Connection: Keep-Alive)；为了获得更好的性能，HTTP 客户端库也可能会管理连接池。请不要通过相同的 TCP 连接传递请求。如需了解详情，请参阅 [HTTP 和 TCP](https://github.com/httpwg/wiki/wiki/TCP)。\n\n现代浏览器都使用 SPDY/*、HTTP/2 或 QUIC 进行通信，它们通过单个信道多路传输请求。除非服务器实现限制来自单个客户端的并发 HTTP 请求数 例如，针对单个源的 100 个 HTTP/2 信息流），否则传统的连接限制 (2-10) 永远不应该成为问题。\n\n### HTTPS\n\n客户端可以通过 API 规范支持的 HTTPS 或 HTTP 访问 API。TLS 协商和 TLS 版本对客户端应用是透明的。默认情况下，Google API 仅接受 HTTPS 流量。\n\n## 请求/响应格式\n\n### 请求网址\n\nJSON-REST 映射支持网址编码形式的请求数据，HTTP 请求和响应主体使用 application/json 作为 Content-Type。HTTP 主体使用 JSON 数组来支持流式 RPC 方法，JSON 数组可能包含任意数量的 JSON 消息或错误状态 JSON 消息。\n\n### 长请求网址\n\n网址具有实际长度限制，通常在 2k 到 8k 之间。此限制由一些浏览器和代理强制执行。如果您的 API 使用网址超过长度限制的 GET 请求，浏览器可能会拒绝此类请求。要避开此限制，客户端代码应使用 Content-Type 为 application/x-www-form-urlencoded 的 POST 请求，且 HTTP 标头为 X-HTTP-Method-Override: GET。该方法也适用于 DELETE 请求。\n\n### HTTP 方法（动词）\n\n如果请求网址符合 REST 模型，则将其 HTTP 方法指定为 API 规范的一部分。特别是，每种 API 方法都必须符合基于 API 方法映射到的特定 HTTP 动词的 HTTP 协议的要求。如需了解详情，请参阅[超文本传输协议](https://datatracker.ietf.org/doc/html/rfc2616#section-9)规范和 [PATCH 方法](https://datatracker.ietf.org/doc/html/rfc5789) RFC。\n\n[安全方法](https://datatracker.ietf.org/doc/html/rfc2616#section-9.1.1)（如 HTTP GET 和 HEAD）不应表示检索以外的操作。具体来说，HTTP GET 应该安全，不应对客户端产生任何显而易见的副作用。\n\nHTTP 中的[幂等性](https://datatracker.ietf.org/doc/html/rfc2616#section-9.1.2)意味着多个相同请求产生的副作用与单个请求相同。GET、PUT 和 DELETE 是与风格指南相关的幂等 HTTP 方法。请注意，幂等性仅通过服务器副作用的形式表示，并未指定有关响应的任何内容。特别是对于不存在的资源，DELETE 应该返回 404 (Not Found)。\n\nHTTP POST 和 PATCH 既不安全也不具有幂等性。（PATCH 在 [RFC 5789](https://datatracker.ietf.org/doc/html/rfc5789) 中引入）\n\n|  HTTP 动词   | 安全  | 幂等   |\n|  ----  | ----  | ----  |\n| [GET](https://datatracker.ietf.org/doc/html/rfc2616#section-9.3)  | 是 | 是 |\n| [PUT](https://datatracker.ietf.org/doc/html/rfc2616#section-9.6)  | - | 是 |\n| [DELETE](https://datatracker.ietf.org/doc/html/rfc2616#section-9.7)  | - | 是 |\n| [POST](https://datatracker.ietf.org/doc/html/rfc2616#section-9.5)  | - | - |\n| [PATCH](https://datatracker.ietf.org/doc/html/rfc5789)  | - | - |\n\n### 负载格式\n\n* 请求和响应应共享相同的 Content-Type，除非请求是带有 application/x-www-form-urlencoded 主体的 GET 或 POST。\n* JSON 在 application/json MIME 类型中受支持。从 proto3 到 JSON 的映射在 [JSON 映射](https://developers.google.com/protocol-buffers/docs/proto3?hl=en#json)中正式指定。\n* 根据用于将请求字段映射到查询参数的相同 REST 样式映射规则，可以使用表单参数 (POST) 代替网址查询参数 (GET)。支持的 Content-Type 为 application/x-www-form-urlencoded。\n\n## 流式\n\n### 半双工与全双工\n\nHTTP 是一种请求响应协议，允许通过面向信息流的不同传输协议（如 TCP (HTTP/1.x)）或其多路复用变体 (SPDY、HTTP/2、QUIC) 传递其请求或响应主体。\n\n作为客户端开发人员，您的应用可以采用流式传输模式生成请求主体，即客户端流式传输。同样地，应用也可以采用流式传输模式使用响应主体，即服务器流式传输。\n\n但是，HTTP 规范未指定在请求正文仍处于挂起状态时是否允许服务器流式传输回响应正文（错误响应除外）。这种语义称为全双工流式传输。尽管许多 HTTP 客户端/服务器/代理软件确实支持全双工流式传输，即使 HTTP/1.1 也支持，但为了避免出现任何互操作性问题，基于 HTTP 的 Cloud API 仅限于半双工流式传输。\n\n默认情况下，Cloud API 中的出价 Bidi 流式传输方法会采用全双工语义。 也就是说，使用 HTTP 调用这样的方法是不安全的。如果流式传输方法仅为半双工（由服务器强制执行），则 API 文档应明确指定半双工行为。\n\n对于浏览器客户端，标准 HTTP 语义进一步受浏览器网络 API 的限制。目前，浏览器仅通过 XHR 或 Fetch 支持服务器流式传输通常遵循传输级别的帧处理方式）。Fetch API 可以使用 [whatwg 流](https://github.com/whatwg/streams)。\n\n由于浏览器的限制，需要浏览器支持的 Cloud API 必须避免采用客户端流式传输以及全双工流式传输，或者为浏览器客户端提供单独的 API。\n\n一般来说，通过互联网进行客户端流式传输不如服务器流式传输有用。这是因为使用客户端流式传输通常会导致有状态服务，这会对负载平衡造成负面影响，让系统更容易出现故障或受到攻击。另一方面，服务器流式传输可能很有用，因为它可以显着减少有长时间 RTT 延迟的网络上的延迟时间。\n\n### 消息编码\n\nJSON 消息在流式传输时将作为 JSON 消息数组进行编码。请求或响应正文将作为有效的 JSON MIME 类型保留。\n\n客户端流式传输编码示例：\n```\n1 <length> <message-bytes> 1 <length> <message-bytes>  … EOF\n```\n\n服务器流式传输编码示例：\n```\n1 <length> <message-bytes>  … 2 <length> <status-bytes> EOF\n```\n\n线级编码：StreamBody 的定义仅在其为字段“messages”分配 tag-id 时才有意义，而“status”对于正常消息将采用 1-2 字节的 varint 编码，因此总的编码开销为每条消息 2-3 个字节\n\n需要添加可选的填充字段，才能支持使用 base64 编码的流：\n```\nmessage StreamBody {\n  repeated bytes message = 1;\n  google.rpc.Status status = 2;\n  repeated bytes padding = 15;   // max one-byte tag-id: xxx01111\n}\n```\n\n错误消息应作为 JSON 或 protobuf 数组的最后一个元素附加，其格式与常规消息相同。\n\n## 状态管理\n\n适用于客户端或服务器的任何 HTTP 版本中都很好地定义了半关闭行为，旨在向另一端发送主体已完成的信号。\n\n特别是，客户端代码在等待响应时可以自由完成请求。与之类似，当请求主体仍在写入服务器时，客户端可能会看到完成的响应。HTTP 标准要求客户端在以意外方式完成响应时（通常具有错误状态）中止或完成请求。这就是说，在正常情况下，服务器不应在客户端仍在发送请求时完成响应。\n\n### 取消\n\n通过取消支持，客户端能够在请求或响应仍在等待处理时中止请求。\n\nHTTP/1.* 客户端没有可靠的取消支持，因为客户端可在请求完成后随时关闭 TCP 连接，而不会中止请求/响应事务。采用 HTTP/1.1 的 TCP FIN 不应被解释为取消，即使连接被标记为保持 keep-alive (Connection: Keep-Alive) 也是如此。\n\n但在客户端关闭 TCP 连接后，如果服务器尝试将任何数据写入客户端，则会生成 RST，这会触发取消。\n\n另请注意，取消也是非流式传输 API 存在的问题。当响应涉及长轮询并且因此导致连接长时间保持空闲时，此问题尤其明显。\n\nSPDY、HTTP/2 和 QUIC 支持显式取消，尤其是与 go-away 消息结合使用。\n\n### Keep-alive\n\n通过 Keep-alive 支持，客户端或服务器能够检测到失败的对等体，即使在出现丢包或网络故障的情况下也是如此。\n\nHTTP/1.1 不提供 keep-alive 支持，因为 TCP keep-alive 不是一种可行的方法。\n\nQUIC 或 HTTP/2 提供特殊控制消息，旨在实现由应用（包括浏览器）提供的 keep-alive 支持。\n\n但是，实现可靠的 keep-alive 和故障检测将可能需要有一个客户端库，加上必要的服务器端支持：如果依赖将基本 HTTP 作为通信协议，通过互联网进行长期流式传输通常容易出错。\n\n\n## 流控制\n\n流控制支持要求客户端将传输级流控制事件传播到客户端应用。实际的机制取决于客户端应用所使用的 HTTP 客户端 API 执行机制。例如，为了防止客户端或服务器过载，您需要使用显式流控制支持阻塞写入和读取或者非阻塞读取和写入，以便应用处理和遵循流控制事件。\n\nHTTP/1.1 依赖于 TCP 流控制。\n\nSPDY 和 HTTP/2 在流级别具有自己的流控制，由于请求通过单个 TCP 连接进行多路复用，因此它们在连接级别进一步受到 TCP 流控制的限制。\n\nQUIC 在 UDP 上运行，因此可以完全自行管理流控制。","tags":["http","Google API"],"categories":["http"]},{"title":"git 提交信息规范","url":"/2021/04/11/git-commit-message/","content":"\n## 您可以遵循的Git commit消息约定！\n\n典型的git commit消息像下面这样\n```\n<type>(<scope>): <subject>\n```\n\n### “类型”必须是以下提到的以下内容之一！\n\n<!-- more -->\n\n* build: 建立相关的更改（例如：与npm相关/添加外部依赖项）\n* chore: 外部用户看不到的代码更改（例如：更改为.gitignore文件或.prettierrc文件）\n* feat: 新特性\n* fix: bug修复\n* docs: 与文档相关的改变\n* refactor: 既不修复错误也不添加功能的代码。 （例如：您可以在语义更改（例如重命名变量/函数名称）时使用此功能）\n* perf: 可以提高性能的代码\n* style: 有关样式的代码\n* test: 添加新测试或对现有测试进行更改\n* update: 更新\n* upgrade: 升级\n* sponsors: 赞助相关\n\n### \"scope\" 是可选的\n\n* scope 必须是名词，并且代表代码库部分的部分\n* [请参阅此链接以获取与scope相关的示例](http://karma-runner.github.io/1.0/dev/git-commit-msg.html)\n\n### \"subject\"\n\n* 使用命令式，现在时(如： 使用\"add\" 代替 \"added\" 或 \"adds\")\n* 不要使用 \".\" 结尾\n* 不要使用首字母大写方式\n\n**[请参阅此链接以获取更多实用的提交消息示例](https://github.com/eslint/eslint/commits/master)**\n\n### References:\n* https://www.conventionalcommits.org/en/v1.0.0/\n* https://gist.github.com/joshbuchea/6f47e86d2510bce28f8e7f42ae84c716\n* https://github.com/fteem/git-semantic-commits","tags":["git"],"categories":["mysql"]},{"title":"Hello World","url":"/2021/03/27/hello-world/","content":"Welcome to [Hexo](https://hexo.io/)! This is your very first post. Check [documentation](https://hexo.io/docs/) for more info. If you get any problems when using Hexo, you can find the answer in [troubleshooting](https://hexo.io/docs/troubleshooting.html) or you can ask me on [GitHub](https://github.com/hexojs/hexo/issues).\n\n## Quick Start\n\n### Create a new post\n\n``` bash\n$ hexo new \"My New Post\"\n```\n\nMore info: [Writing](https://hexo.io/docs/writing.html)\n\n<!-- more -->\n\n### Run server\n\n``` bash\n$ hexo server\n```\n\nMore info: [Server](https://hexo.io/docs/server.html)\n\n### Generate static files\n\n``` bash\n$ hexo generate\n```\n\nMore info: [Generating](https://hexo.io/docs/generating.html)\n\n### Deploy to remote sites\n\n``` bash\n$ hexo deploy\n```\n\nMore info: [Deployment](https://hexo.io/docs/one-command-deployment.html)\n"},{"title":"http vs https","url":"/2021/03/27/https/","content":"Welcome to [Hexo](https://hexo.io/)! This is your very first post. Check [documentation](https://hexo.io/docs/) for more info. If you get any problems when using Hexo, you can find the answer in [troubleshooting](https://hexo.io/docs/troubleshooting.html) or you can ask me on [GitHub](https://github.com/hexojs/hexo/issues).\n\n## Quick Start\n\n### Create a new post\n\n``` bash\n$ hexo new \"My New Post\"\n```\n\nMore info: [Writing](https://hexo.io/docs/writing.html)\n\n<!-- more -->\n\n### Run server\n\n``` bash\n$ hexo server\n```\n\nMore info: [Server](https://hexo.io/docs/server.html)\n\n### Generate static files\n\n``` bash\n$ hexo generate\n```\n\nMore info: [Generating](https://hexo.io/docs/generating.html)\n\n### Deploy to remote sites\n\n``` bash\n$ hexo deploy\n```\n\nMore info: [Deployment](https://hexo.io/docs/one-command-deployment.html)\n"},{"title":"golang-gmp","url":"/2021/03/27/golang/","content":"\nthis is golang new test\n","categories":["golang"]},{"title":"索引为何使用B+树","url":"/2020/10/12/索引为何使用B+树/","content":"\n## 问题思考\n\n数据库索引的数据结构有很多种，比如：哈希索引、平衡二叉树索引、B树索引、B+树索引等等。\n\n目前最流行的是B+树索引，那大家有没有想过为什么是B+树索引最流行，为什么其他索引应用不广泛\n\n## 哈希索引\n\n<!-- more -->\n\nhash大家应该非常的熟悉，就是我们老生常谈的HashMap里用到的技术。Hash索引其检索效率非常高，索引的检索可以一次定位。\n\n既然Hash索引的效率这么高，为什么都用Hash索引而还要使用B-Tree索引呢?\n\n这是因为虽然Hash索引效率高，但是Hash索引本身由于其特殊性也带来了很多限制和弊端，主要有以下这些：\n\n### 1. 原因一：\n\n**Hash索引不能使用范围查询**\n\nHash索引仅仅能满足\"=\",\"IN\"和\"<=>\"查询(注意<>和＜＝＞是不同的操作），不能使用范围查询，例如WHERE price > 100。\n\n由于Hash索引比较的是进行Hash运算之后的Hash值，所以它只能用于等值的过滤，不能用于基于范围的过滤。\n\n### 2. 原因二：\n\n**Hash索引不能利用部分索引键查询。**\n\n对于复合索引，Hash索引在计算Hash值的时候，是组合索引键合并后再一起计算Hash值，而不是单独计算Hash值。\n\n所以通过复合索引的前面一个或几个索引键进行查询的时候，Hash索引也无法被利用。\n\n### 3. 原因三：\n\n**Hash索引在任何时候都不能避免表扫描。**\n\nHash索引是将索引键通过Hash运算之后，将 Hash运算结果的Hash值和所对应的行指针信息存放于一个Hash表中。\n\n由于不同索引键存在相同Hash值，所以无法从Hash索引中直接完成查询，还是要通过访问表中的实际数据进行相应的比较，并得到相应的结果。\n\n\n## 平衡二叉树索引\n\n平衡二叉树的结构特点：\n\n![mysql_avl_tree](https://luckylsx.github.io/images/mysql/mysql_avl_tree.png)\n\n又称 AVL树。它除了具备二叉查找树的基本特征之外，还具有一个非常重要的特点：它的左子树和右子树都是平衡二叉树。\n\n且左子树和右子树的深度之差的绝对值（平衡因子 ）不超过1。也就是说AVL树每个节点的平衡因子只可能是-1、0和1（左子树高度减去右子树高度）。\n\n### 被淘汰的原因:\n\n- 树的高度过高，高度越高，查找速度越慢\n- 他支持范围查找，但是他需要在进行回旋查找\n\n比如我要找到大于5的数据\n\n第一步我先定位到5，然后在树上按照二叉树规则去回旋查找大于5其他数据6、7、8、9、10。。。\n\n如果大于5的数据很多，那速度是很慢的。\n\n\n## B树索引\n\n![mysql_b_tree](https://luckylsx.github.io/images/mysql/mysql_b_tree.png)\n\n大家可以看到B树和二叉树最大的区别在于：它一个节点可以存储两个值，这就意味着它的树高度，比二叉树的高度更低，它的查询速度就更快。这是他的优点\n\n那为什么最终还是不用它呢，还是因为他在范围查找的时候，存在回旋查询的问题。同样order by排序的时候效率也很低，因为要把树上的数据手动排序一遍。\n\n## 终极大佬：B+树:\n\n![mysql_b_plus_tree](https://luckylsx.github.io/images/mysql/mysql_b_plus_tree.png)\n\n它是B数的升级版，B+树相比B树，新增叶子节点与非叶子节点关系。\n\n叶子节点中包含了key和value，key存储的是1-10这些数字，value存储的是数据存储地址，非叶子节点中只是包含了key，不包含value。\n\n所有相邻的叶子节点包含非叶子节点，使用链表进行结合，有一定顺序排序，从而范围查询效率非常高\n\n### 比如我们要查找大于5的数据：\n\n- 首先我们定位到5的位置\n- 然后直接将5后面的数据全部拿出来即可，因为这是有序链表，已经排好序了\n\n我们在order by排序的时候为什么要使用索引进行排序，原因就在这。","tags":["mysql","B+树","hash"],"categories":["mysql"]},{"title":"go实现各类限流算法","url":"/2020/05/30/go实现各类限流算法/","content":"\n## 介绍\n限流是一种编程中常用的保护服务正常运行的一种方案；为了应对激增的流量，防止服务被击垮，经常使用某种算法来对流量进行某种限流，以保证后端服务的正常运行。\n\n本文介绍 计数器、令牌桶算法和楼桶算法及它们之间的优缺点。\n\n## 计数器\n```\npackage main\n\nimport (\n\t\"log\"\n\t\"sync\"\n\t\"time\"\n)\n\n// Counter Counter\ntype Counter struct {\n\trate  int\n\tbegin time.Time\n\tcycle time.Duration\n\tcount int\n\tlock  sync.Mutex\n}\n\n// Set Set\nfunc (c *Counter) Set(r int, cycle time.Duration) {\n\tc.rate = r\n\tc.begin = time.Now()\n\tc.cycle = cycle\n\tc.count = 0\n}\n\n// Reset Reset\nfunc (c *Counter) Reset(t time.Time) {\n\tc.begin = t\n\tc.count = 0\n}\n\n// Allow Allow\nfunc (c *Counter) Allow() bool {\n\tc.lock.Lock()\n\tdefer c.lock.Unlock()\n\n\tif c.count == c.rate {\n\t\tnow := time.Now()\n\t\tif now.Sub(c.begin) >= c.cycle {\n\t\t\tc.Reset(now)\n\t\t\treturn true\n\t\t} else {\n\t\t\treturn false\n\t\t}\n\t} else {\n\t\tc.count++\n\t\treturn true\n\t}\n\n}\n\nfunc main() {\n\tvar wg sync.WaitGroup\n\tlr := new(Counter)\n\tlr.Set(3, time.Second)\n\tfor i := 0; i < 10; i++ {\n\t\twg.Add(1)\n\t\tlog.Println(\"创建请求:\", i)\n\t\tgo func(i int) {\n\t\t\tif lr.Allow() {\n\t\t\t\tlog.Println(\"request allw: \", i)\n\t\t\t}\n\t\t\twg.Done()\n\t\t}(i)\n\t\ttime.Sleep(200 * time.Millisecond)\n\t}\n\twg.Wait()\n}\n```\n\n这个算法虽然简单，但是有一个十分致命的问题，那就是临界问题，我们看下图：\n![counter](/images/golang/limit/counter.png)\n从上图中我们可以看到，假设有一个恶意用户，他在0:59时，瞬间发送了100个请求，并且1:00又瞬间发送了100个请求，那么其实这个用户在 1秒里面，瞬间发送了200个请求。我们刚才规定的是1分钟最多100个请求，也就是每秒钟最多1.7个请求，用户通过在时间窗口的重置节点处突发请求， 可以瞬间超过我们的速率限制。用户有可能通过算法的这个漏洞，瞬间压垮我们的应用。\n## 令牌桶算法\n\n是一个存放固定容量令牌的桶，按照固定速率往桶里添加令牌。令牌桶算法的描述如下：\n\n![token-bucket](/images/golang/limit/token-bucket.png)\n\n* 假设限制2r/s，则按照500毫秒的固定速率往桶中添加令牌。\n* 桶中最多存放 b 个令牌，当桶满时，新添加的令牌被丢弃或拒绝。\n* 当一个 n 个字节大小的数据包到达，将从桶中删除n 个令牌，接着数据包被发送到网络上。\n* 如果桶中的令牌不足 n 个，则不会删除令牌，且该数据包将被限流（要么丢弃，要么缓冲区等待）。\n\n官方实现：https://pkg.go.dev/golang.org/x/time/rate\n\n### 通过 time/rate 包实现\n```\n// 构建限流对象\n// NewLimiter returns a new Limiter that allows events up to rate r and permits\n// bursts of at most b tokens.\nfunc NewLimiter(r Limit, b int) *Limiter {\n    return &Limiter{\n        limit: r,\n        burst: b,\n    }\n}\n```\n\nAllow/AllowN 判断是否允许通过\n```\n// Allow is shorthand for AllowN(time.Now(), 1).\nfunc (lim *Limiter) Allow() bool {\n    return lim.AllowN(time.Now(), 1)\n}\n\n// AllowN reports whether n events may happen at time now.\n// Use this method if you intend to drop / skip events that exceed the rate limit.\n// Otherwise use Reserve or Wait.\nfunc (lim *Limiter) AllowN(now time.Time, n int) bool {\n    return lim.reserveN(now, n, 0).ok\n}\n```\n具体细节，请看官方实现。\n\n### 自己手动实现\n\ndemo示例：\n```\npackage main\n\nimport (\n\t\"math\"\n\t\"sync\"\n\t\"time\"\n)\n\ntype tokenBucket struct {\n\trate     int64 // 速率\n\tcapacity int64 // 桶容量\n\ttokens   int64 // 桶内token 个数\n\tlastTime time.Time\n\tlock sync.Mutex\n}\n\nfunc (t *tokenBucket) allow() bool {\n\tt.lock.Lock()\n\tdefer t.lock.Unlock()\n\tnow := time.Now()\n\tt.tokens = t.tokens + int64(now.Sub(t.lastTime).Seconds() * float64(t.rate))\n\tt.tokens = int64(math.Min(float64(t.tokens), float64(t.capacity)))\n\tif t.tokens > 0 {\n\t\tt.lastTime = now\n\t\tt.tokens--\n\t\treturn true\n\t}\n\treturn false\n}\n\nfunc NewTokenBucket(r, c int64) *tokenBucket {\n\treturn &tokenBucket{\n\t\trate:     r,\n\t\tcapacity: c,\n\t\ttokens:   0,\n\t\tlastTime: time.Now(),\n\t}\n}\n```\n\n测试，此时每5个请求，一个请求成功\n```\nfunc main() {\n\tt := NewTokenBucket(2, 5)\n\tfor i := 0; i < 20; i++ {\n\t\ttime.Sleep(time.Millisecond * 100)\n\t\tif ok := t.allow(); ok {\n\t\t\tfmt.Println(\"request allow!\")\n\t\t} else {\n\t\t\tfmt.Println(\"request deny!\")\n\t\t}\n\t}\n}\n\nrequest deny!\nrequest deny!\nrequest deny!\nrequest deny!\nrequest allow!\n...\n```\n\n## 漏桶算法\n\n作为计量工具(The Leaky Bucket Algorithm as a Meter)时，可以用于流量整形(Traffic Shaping)和流量控制(TrafficPolicing)，漏桶算法的描述如下：\n\n* 一个固定容量的漏桶，按照常量固定速率流出水滴。\n* 如果桶是空的，则不需流出水滴。\n* 可以以任意速率流入水滴到漏桶。\n* 如果流入水滴超出了桶的容量，则流入的水滴溢出了（被丢弃），而漏桶容量是不变的。\n\nleaky-bucket rate limit algorithm: \nhttps://pkg.go.dev/go.uber.org/ratelimit\n\n### uber ratelimit 官方实现\n```\nimport (\n\t\"fmt\"\n\t\"time\"\n\n\t\"go.uber.org/ratelimit\"\n)\n\nfunc main() {\n    rl := ratelimit.New(100) // per second\n\n    prev := time.Now()\n    for i := 0; i < 10; i++ {\n        // 此处会被阻塞，直到请求允许通过\n        // Take should block to make sure that the RPS is met.\n        now := rl.Take()\n        fmt.Println(i, now.Sub(prev))\n        prev = now\n    }\n\n    // Output:\n    // 0 0\n    // 1 10ms\n    // 2 10ms\n    // 3 10ms\n    // 4 10ms\n    // 5 10ms\n    // 6 10ms\n    // 7 10ms\n    // 8 10ms\n    // 9 10ms\n}\n```\n\n### 自己实现\n```\npackage main\n\nimport (\n\t\"math\"\n\t\"sync\"\n\t\"time\"\n)\n\ntype LeakBucket struct {\n\trate       float64 // 固定每秒出水率\n\tcapacity   float64 // 桶容量\n\twater      float64 // 桶中当前水量\n\tlastLeakMs int64   // 上次漏水时间戳\n\tlock       sync.Mutex\n}\n\nfunc (l *LeakBucket) Allow() bool {\n\tl.lock.Lock()\n\tdefer l.lock.Unlock()\n\n\tnow := time.Now().UnixNano() / 1e6\n\teclipse := float64(now-l.lastLeakMs) * l.rate / 1000\n\tl.water = l.water - eclipse\n\tl.water = math.Max(0, l.water)\n\tif (l.water + 1) < l.capacity {\n\t\tl.lastLeakMs = now\n\t\tl.water++\n\t\treturn true\n\t} else {\n\t\treturn false\n\t}\n}\n\nfunc newBucket(r float64, c float64) *LeakBucket {\n\treturn &LeakBucket{\n\t\trate:       r,\n\t\tcapacity:   c,\n\t\twater:      0,\n\t\tlastLeakMs: time.Now().UnixNano() / 1e6,\n\t}\n}\n```\n\n计数器\n漏桶算法 流出的速率是固定的，与令牌桶算法不同的是，令牌桶算法是放入的速率固定，取出的速率不固定，所以令牌桶算法支持流量的爆发。\n\n\n\n","tags":["golang","limiter"],"categories":["golang"]},{"title":"MySQL数据库设计规范","url":"/2020/04/13/the-Mysql-Standards-Recommendation/","content":"\n## 目录\n\n```\n1. 规范背景与目的\t\n\n2. 设计规范\n\n2.1 数据库设计\t\n\n2.1.1 库名\t\n2.1.2 表结构\t\n2.1.3 列数据类型优化\t\n2.1.4 索引设计\t\n2.1.5 分库分表、分区表\t\n2.1.6 字符集\t\n2.1.7 程序DAO层设计建议\t\n2.1.8 一个规范的建表语句示例\t\n\n2.2 SQL编写\t\n\n2.2.1 DML语句\t\n2.2.2 多表连接\t\n2.2.3 事务\t\n2.2.4 排序和分组\t\n2.2.5 线上禁止使用的SQL语句\n```\n\n<!-- more -->\n\n\n## 1. 规范背景与目的\nMySQL数据库与 Oracle、 SQL Server 等数据库相比，有其内核上的优势与劣势。我们在使用MySQL数据库的时候需要遵循一定规范，扬长避短。本规范旨在帮助或指导RD、QA、OP等技术人员做出适合线上业务的数据库设计。在数据库变更和处理流程、数据库表设计、SQL编写等方面予以规范，从而为公司业务系统稳定、健康地运行提供保障。\n\n## 2. 设计规范\n\n### 2.1 数据库设计\n以下所有规范会按照【高危】、【强制】、【建议】三个级别进行标注，遵守优先级从高到低。\n\n对于不满足【高危】和【强制】两个级别的设计，DBA会强制打回要求修改。\n\n#### 2.1.1 库名\n1.【强制】库的名称必须控制在32个字符以内，相关模块的表名与表名之间尽量提现join的关系，如user表和user_login表。\n\n2.【强制】库的名称格式：业务系统名称_子系统名，同一模块使用的表名尽量使用统一前缀。\n\n3.【强制】一般分库名称命名格式是库通配名_编号，编号从0开始递增，比如wenda_001以时间进行分库的名称格式是“库通配名_时间”\n\n4.【强制】创建数据库时必须显式指定字符集，并且字符集只能是utf8或者utf8mb4。创建数据库SQL举例：create database db1 default character set utf8;。\n\n#### 2.1.2 表结构\n\n1.【强制】表和列的名称必须控制在32个字符以内，表名只能使用字母、数字和下划线，一律小写。\n\n2.【强制】表名要求模块名强相关，如师资系统采用”sz”作为前缀，渠道系统采用”qd”作为前缀等。\n\n3.【强制】创建表时必须显式指定字符集为utf8或utf8mb4。\n\n4.【强制】创建表时必须显式指定表存储引擎类型，如无特殊需求，一律为InnoDB。当需要使用除InnoDB/MyISAM/Memory以外的存储引擎时，必须通过DBA审核才能在生产环境中使用。因为Innodb表支持事务、行锁、宕机恢复、MVCC等关系型数据库重要特性，为业界使用最多的MySQL存储引擎。而这是其他大多数存储引擎不具备的，因此首推InnoDB。\n\n5.【强制】建表必须有comment\n\n6.【建议】建表时关于主键：(1)强制要求主键为id，类型为int或bigint，且为auto_increment。(2)标识表里每一行主体的字段不要设为主键，建议设为其他字段如user_id，order_id等，并建立unique key索引（可参考cdb.teacher表设计）。因为如果设为主键且主键值为随机插入，则会导致innodb内部page分裂和大量随机I/O，性能下降。\n\n7.【建议】核心表（如用户表，金钱相关的表）必须有行数据的创建时间字段create_time和最后更新时间字段update_time，便于查问题。\n\n8.【建议】表中所有字段必须都是NOT NULL属性，业务可以根据需要定义DEFAULT值。因为使用NULL值会存在每一行都会占用额外存储空间、数据迁移容易出错、聚合函数计算结果偏差等问题。\n\n9.【建议】建议对表里的blob、text等大字段，垂直拆分到其他表里，仅在需要读这些对象的时候才去select。\n\n10.【建议】反范式设计：把经常需要join查询的字段，在其他表里冗余一份。如user_name属性在user_account，user_login_log等表里冗余一份，减少join查询。\n\n11.【强制】中间表用于保留中间结果集，名称必须以tmp_开头。备份表用于备份或抓取源表快照，名称必须以bak_开头。中间表和备份表定期清理。\n\n12.【强制】对于超过100W行的大表进行alter table，必须经过DBA审核，并在业务低峰期执行。因为alter table会产生表锁，期间阻塞对于该表的所有写入，对于业务可能会产生极大影响。\n\n#### 2.1.3 列数据类型优化\n\n1.【建议】表中的自增列（auto_increment属性），推荐使用bigint类型。因为无符号int存储范围为-2147483648~2147483647（大约21亿左右），溢出后会导致报错。\n\n2.【建议】业务中选择性很少的状态status、类型type等字段推荐使用tinytint或者smallint类型节省存储空间。\n\n3.【建议】业务中IP地址字段推荐使用int类型，不推荐用char(15)。因为int只占4字节，可以用如下函数相互转换，而char(15)占用至少15字节。一旦表数据行数到了1亿，那么要多用1.1G存储空间。SQL：select inet_aton('192.168.2.12'); select inet_ntoa(3232236044); PHP: ip2long(‘192.168.2.12’); long2ip(3530427185);\n\n4.【建议】不推荐使用enum，set。因为它们浪费空间，且枚举值写死了，变更不方便。推荐使用tinyint或smallint。\n\n5.【建议】不推荐使用blob，text等类型。它们都比较浪费硬盘和内存空间。在加载表数据时，会读取大字段到内存里从而浪费内存空间，影响系统性能。建议和PM、RD沟通，是否真的需要这么大字段。Innodb中当一行记录超过8098字节时，会将该记录中选取最长的一个字段将其768字节放在原始page里，该字段余下内容放在overflow-page里。不幸的是在compact行格式下，原始page和overflow-page都会加载。\n\n6.【建议】存储金钱的字段，建议用int，程序端乘以100和除以100进行存取。因为int占用4字节，而double占用8字节，空间浪费。\n\n7.【建议】文本数据尽量用varchar存储。因为varchar是变长存储，比char更省空间。MySQL server层规定一行所有文本最多存65535字节，因此在utf8字符集下最多存21844个字符，超过会自动转换为mediumtext字段。而text在utf8字符集下最多存21844个字符，mediumtext最多存2^24/3个字符，longtext最多存2^32个字符。一般建议用varchar类型，字符数不要超过2700。\n\n8.【建议】时间类型尽量选取timestamp。因为datetime占用8字节，timestamp仅占用4字节，但是范围为1970-01-01 00:00:01到2038-01-01 00:00:00。更为高阶的方法，选用int来存储时间，使用SQL函数unix_timestamp()和from_unixtime()来进行转换。\n\n详细存储大小参加下图：\n\n![mysql_time_base](https://luckylsx.github.io/images/mysql/mysql_time_type.png)\n\n#### 2.1.4 索引设计\n\n1.【强制】InnoDB表必须主键为id int/bigint auto_increment,且主键值禁止被更新。\n\n2.【建议】主键的名称以“pk_”开头，唯一键以“uk_”或“uq_”开头，普通索引以“idx_”开头，一律使用小写格式，以表名/字段的名称或缩写作为后缀。\n\n3.【强制】InnoDB和MyISAM存储引擎表，索引类型必须为BTREE；MEMORY表可以根据需要选择HASH或者BTREE类型索引。\n\n4.【强制】单个索引中每个索引记录的长度不能超过64KB。\n\n5.【建议】单个表上的索引个数不能超过7个。\n\n6.【建议】在建立索引时，多考虑建立联合索引，并把区分度最高的字段放在最前面。如列userid的区分度可由select count(distinct userid)计算出来。\n\n7.【建议】在多表join的SQL里，保证被驱动表的连接列上有索引，这样join执行效率最高。\n\n8.【建议】建表或加索引时，保证表里互相不存在冗余索引。对于MySQL来说，如果表里已经存在key(a,b)，则key(a)为冗余索引，需要删除。\n\n#### 2.1.5 分库分表、分区表\n\n1.【强制】分区表的分区字段（partition-key）必须有索引，或者是组合索引的首列。\n\n2.【强制】单个分区表中的分区（包括子分区）个数不能超过1024。\n\n3.【强制】上线前RD或者DBA必须指定分区表的创建、清理策略。\n\n4.【强制】访问分区表的SQL必须包含分区键。\n\n5.【建议】单个分区文件不超过2G，总大小不超过50G。建议总分区数不超过20个。\n\n6.【强制】对于分区表执行alter table操作，必须在业务低峰期执行。\n\n7.【强制】采用分库策略的，库的数量不能超过1024\n\n8.【强制】采用分表策略的，表的数量不能超过4096\n\n9.【建议】单个分表不超过500W行，ibd文件大小不超过2G，这样才能让数据分布式变得性能更佳。\n\n10.【建议】水平分表尽量用取模方式，日志、报表类数据建议采用日期进行分表。\n\n#### 2.1.6 字符集\n\n1.【强制】数据库本身库、表、列所有字符集必须保持一致，为utf8或utf8mb4。\n\n2.【强制】前端程序字符集或者环境变量中的字符集，与数据库、表的字符集必须一致，统一为utf8。\n\n#### 2.1.7 程序层DAO设计建议\n\n1.【建议】新的代码不要用model，推荐使用手动拼SQL+绑定变量传入参数的方式。因为model虽然可以使用面向对象的方式操作db，但是其使用不当很容易造成生成的SQL非常复杂，且model层自己做的强制类型转换性能较差，最终导致数据库性能下降。\n\n2.【建议】前端程序连接MySQL或者redis，必须要有连接超时和失败重连机制，且失败重试必须有间隔时间。\n\n3.【建议】前端程序报错里尽量能够提示MySQL或redis原生态的报错信息，便于排查错误。\n\n4.【建议】对于有连接池的前端程序，必须根据业务需要配置初始、最小、最大连接数，超时时间以及连接回收机制，否则会耗尽数据库连接资源，造成线上事故。\n\n5.【建议】对于log或history类型的表，随时间增长容易越来越大，因此上线前RD或者DBA必须建立表数据清理或归档方案。\n\n6.【建议】在应用程序设计阶段，RD必须考虑并规避数据库中主从延迟对于业务的影响。尽量避免从库短时延迟（20秒以内）对业务造成影响，建议强制一致性的读开启事务走主库，或更新后过一段时间再去读从库。\n\n7.【建议】多个并发业务逻辑访问同一块数据（innodb表）时，会在数据库端产生行锁甚至表锁导致并发下降，因此建议更新类SQL尽量基于主键去更新。\n\n8.【建议】业务逻辑之间加锁顺序尽量保持一致，否则会导致死锁。\n\n9.【建议】对于单表读写比大于10:1的数据行或单个列，可以将热点数据放在缓存里（如mecache或redis），加快访问速度，降低MySQL压力。\n\n#### 2.1.8 一个规范的建表语句示例\n\n**一个较为规范的建表语句为：**\n\n```\nCREATE TABLE user (\n  `id` bigint(11) NOT NULL AUTO_INCREMENT,\n  `user_id` bigint(11) NOT NULL COMMENT ‘用户id’\n  `username` varchar(45) NOT NULL COMMENT '真实姓名',\n  `email` varchar(30) NOT NULL COMMENT ‘用户邮箱’,\n  `nickname` varchar(45) NOT NULL COMMENT '昵称',\n  `avatar` int(11) NOT NULL COMMENT '头像',\n  `birthday` date NOT NULL COMMENT '生日',\n  `sex` tinyint(4) DEFAULT '0' COMMENT '性别',\n  `short_introduce` varchar(150) DEFAULT NULL COMMENT '一句话介绍自己，最多50个汉字',\n  `user_resume` varchar(300) NOT NULL COMMENT '用户提交的简历存放地址',\n  `user_register_ip` int NOT NULL COMMENT ‘用户注册时的源ip’,\n  `create_time` timestamp NOT NULL COMMENT ‘用户记录创建的时间’,\n  `update_time` timestamp NOT NULL COMMENT ‘用户资料修改的时间’,\n  `user_review_status` tinyint NOT NULL COMMENT ‘用户资料审核状态，1为通过，2为审核中，3为未通过，4为还未提交审核’,\n  PRIMARY KEY (`id`),\n  UNIQUE KEY `idx_user_id` (`user_id`),\n  KEY `idx_username`(`username`),\n  KEY `idx_create_time`(`create_time`,`user_review_status`)\n) ENGINE=InnoDB DEFAULT CHARSET=utf8 COMMENT='网站用户基本信息';\n```\n\n### 2.2 SQL编写\n\n#### 2.2.1 DML语句\n\n1.【强制】SELECT语句必须指定具体字段名称，禁止写成*。因为select *会将不该读的数据也从MySQL里读出来，造成网卡压力。且表字段一旦更新，但model层没有来得及更新的话，系统会报错。\n\n2.【强制】insert语句指定具体字段名称，不要写成insert into t1 values(…)，道理同上。\n\n3.【建议】insert into…values(XX),(XX),(XX)…。这里XX的值不要超过5000个。值过多虽然上线很很快，但会引起主从同步延迟。\n\n4.【建议】SELECT语句不要使用UNION，推荐使用UNION ALL，并且UNION子句个数限制在5个以内。因为union all不需要去重，节省数据库资源，提高性能。\n\n5.【建议】in值列表限制在500以内。例如select… where userid in(….500个以内…)，这么做是为了减少底层扫描，减轻数据库压力从而加速查询。\n\n6.【建议】事务里批量更新数据需要控制数量，进行必要的sleep，做到少量多次。\n\n7.【强制】事务涉及的表必须全部是innodb表。否则一旦失败不会全部回滚，且易造成主从库同步终端。\n\n8.【强制】写入和事务发往主库，只读SQL发往从库。\n\n9.【强制】除静态表或小表（100行以内），DML语句必须有where条件，且使用索引查找。\n\n10.【强制】生产环境禁止使用hint，如sql_no_cache，force index，ignore key，straight join等。因为hint是用来强制SQL按照某个执行计划来执行，但随着数据量变化我们无法保证自己当初的预判是正确的，因此我们要相信MySQL优化器！\n\n11.【强制】where条件里等号左右字段类型必须一致，否则无法利用索引。\n\n12.【建议】SELECT|UPDATE|DELETE|REPLACE要有WHERE子句，且WHERE子句的条件必需使用索引查找。\n\n13.【强制】生产数据库中强烈不推荐大表上发生全表扫描，但对于100行以下的静态表可以全表扫描。查询数据量不要超过表行数的25%，否则不会利用索引。\n\n14.【强制】WHERE 子句中禁止只使用全模糊的LIKE条件进行查找，必须有其他等值或范围查询条件，否则无法利用索引。\n\n15.【建议】索引列不要使用函数或表达式，否则无法利用索引。如where length(name)='Admin'或where user_id+2=10023。\n\n16.【建议】减少使用or语句，可将or语句优化为union，然后在各个where条件上建立索\n\n17.【建议】分页查询，当limit起点较高时，可先用过滤条件进行过滤。如select a,b,c from t1 limit 10000,20;优化为: select a,b,c from t1 where id>10000 limit 20;。\n\n#### 2.2.2 多表连接\n\n1.【强制】禁止跨db的join语句。因为这样可以减少模块间耦合，为数据库拆分奠定坚实基础。\n\n2.【强制】禁止在业务的更新类SQL语句中使用join，比如update t1 join t2…。\n\n3.【建议】不建议使用子查询，建议将子查询SQL拆开结合程序多次查询，或使用join来代替子查询。\n\n4.【建议】线上环境，多表join不要超过3个表。\n\n5.【建议】多表连接查询推荐使用别名，且SELECT列表中要用别名引用字段，数据库.表格式，如select a from db1.table1 alias1 where …。\n\n6.【建议】在多表join中，尽量选取结果集较小的表作为驱动表，来join其他表。\n\n#### 2.2.3 事务\n\n1.【建议】事务中INSERT|UPDATE|DELETE|REPLACE语句操作的行数控制在2000以内，以及WHERE子句中IN列表的传参个数控制在500以内。\n\n2.【建议】批量操作数据时，需要控制事务处理间隔时间，进行必要的sleep，一般建议值5-10秒。\n\n3.【建议】对于有auto_increment属性字段的表的插入操作，并发需要控制在200以内。\n\n4.【强制】程序设计必须考虑“数据库事务隔离级别”带来的影响，包括脏读、不可重复读和幻读。线上建议事务隔离级别为repeatable-read。\n\n5.【建议】事务里包含SQL不超过5个（支付业务除外）。因为过长的事务会导致锁数据较久，MySQL内部缓存、连接消耗过多等雪崩问题。\n\n6.【建议】事务里更新语句尽量基于主键或unique key，如update … where id=XX; 否则会产生间隙锁，内部扩大锁定范围，导致系统性能下降，产生死锁。\n\n7.【建议】尽量把一些典型外部调用移出事务，如调用webservice，访问文件存储等，从而避免事务过长。\n\n8.【建议】对于MySQL主从延迟严格敏感的select语句，请开启事务强制访问主库。\n\n#### 2.2.4 排序和分组\n\n1.【建议】减少使用order by，和业务沟通能不排序就不排序，或将排序放到程序端去做。order by、group by、distinct这些语句较为耗费CPU，数据库的CPU资源是极其宝贵的。\n\n2.【建议】order by、group by、distinct这些SQL尽量利用索引直接检索出排序好的数据。如where a=1 order by可以利用key(a,b)。\n\n3.【建议】包含了order by、group by、distinct这些查询的语句，where条件过滤出来的结果集请保持在1000行以内，否则SQL会很慢。\n\n#### 2.2.5 线上禁止使用的SQL语句\n\n1.【高危】禁用update|delete t1 … where a=XX limit XX; 这种带limit的更新语句。因为会导致主从不一致，导致数据错乱。建议加上order by PK。\n\n2.【高危】禁止使用关联子查询，如update t1 set … where name in(select name from user where…);效率极其低下。\n\n3.【强制】禁用procedure、function、trigger、views、event、外键约束。因为他们消耗数据库资源，降低数据库实例可扩展性。推荐都在程序端实现。\n\n4.【强制】禁用insert into …on duplicate key update…在高并发环境下，会造成主从不一致。\n\n5.【强制】禁用insert into …on duplicate key update…在高并发环境下，会造成主从不一致。\n\n","tags":["mysq"],"categories":["MYSQL"]},{"title":"一条sql语句的执行过程","url":"/2020/01/14/一条sql语句的执行过程/","content":"\n比如：\n```\nmysql> select * from T where ID=10；\n```\n要想知道这条语句在 MySQL 内部的执行过程，需要把 MySQL 拆解一下看看里面都有哪些“零件”，希望借由这个拆解过程，让你对 MySQL 有更深入的理解。这样当我们碰到 MySQL 的一些异常或者问题时，就能够直戳本质，更为快速地定位并解决问题。\n\n<!-- more -->\n\n下面我给出的是 MySQL 的基本架构示意图，从中你可以清楚地看到 SQL 语句在 MySQL 的各个功能模块中的执行过程。\n\nMySQL 的逻辑架构图\n\n![mysql_base](/images/mysql/mysql_base.png)\n\n大体来说，MySQL 可以分为 Server 层和存储引擎层。\n\nServer 层包括连接器、查询缓存、分析器、优化器、执行器等，涵盖 MySQL 的大多数核心服务功能，以及所有的内置函数（如日期、时间、数学和加密函数等），所有跨存储引擎的功能都在这一层实现，比如存储过程、触发器、视图等。\n\n而存储引擎层负责数据的存储和提取。其架构模式是插件式的，支持 InnoDB、MyISAM、Memory 等多个存储引擎。现在最常用的存储引擎是 InnoDB，它从 MySQL 5.5.5 版本开始成为了默认存储引擎。\n\n也就是说，你执行 create table 建表的时候，如果不指定引擎类型，默认使用的就是 InnoDB。不过，你也可以通过指定存储引擎的类型来选择别的引擎，比如在 create table 语句中使用 engine=memory, 来指定使用内存引擎创建表。不同存储引擎的表数据存取方式不同，支持的功能也不同。\n\n从图中不难看出，不同的存储引擎共用一个Server 层，也就是从连接器到执行器的部分。你可以先对每个组件的名字有个印象，接下来我会结合开头提到的那条 SQL 语句，带你走一遍整个执行流程，依次看下每个组件的作用。\n\n连接器\n第一步，你会先连接到这个数据库上，这时候接待你的就是连接器。连接器负责跟客户端建立连接、获取权限、维持和管理连接。连接命令一般是这么写的：\n\n```\nmysql -h$ip -P$port -u$user -p\n```\n\n输完命令之后，你就需要在交互对话里面输入密码。虽然密码也可以直接跟在 -p 后面写在命令行中，但这样可能会导致你的密码泄露。如果你连的是生产服务器，强烈建议你不要这么做。\n\n连接命令中的 mysql 是客户端工具，用来跟服务端建立连接。在完成经典的 TCP 握手后，连接器就要开始认证你的身份，这个时候用的就是你输入的用户名和密码。\n\n- 如果用户名或密码不对，你就会收到一个\"Access denied for user\"的错误，然后客户端程序结束执行。\n- 如果用户名密码认证通过，连接器会到权限表里面查出你拥有的权限。之后，这个连接里面的权限判断逻辑，都将依赖于此时读到的权限。\n\n这就意味着，一个用户成功建立连接后，即使你用管理员账号对这个用户的权限做了修改，也不会影响已经存在连接的权限。修改完成后，只有再新建的连接才会使用新的权限设置。\n\n连接完成后，如果你没有后续的动作，这个连接就处于空闲状态，你可以在 show processlist 命令中看到它。文本中这个图是 show processlist 的结果，其中的 Command 列显示为“Sleep”的这一行，就表示现在系统里面有一个空闲连接。\n\n![mysql_processlist](/images/mysql/mysql_processlist.png)\n\n客户端如果太长时间没动静，连接器就会自动将它断开。这个时间是由参数 wait_timeout 控制的，默认值是 8 小时。\n\n如果在连接被断开之后，客户端再次发送请求的话，就会收到一个错误提醒： Lost connection to MySQL server during query。这时候如果你要继续，就需要重连，然后再执行请求了。\n\n数据库里面，长连接是指连接成功后，如果客户端持续有请求，则一直使用同一个连接。短连接则是指每次执行完很少的几次查询就断开连接，下次查询再重新建立一个。\n\n建立连接的过程通常是比较复杂的，所以我建议你在使用中要尽量减少建立连接的动作，也就是尽量使用长连接。\n\n但是全部使用长连接后，你可能会发现，有些时候 MySQL 占用内存涨得特别快，这是因为MySQL 在执行过程中临时使用的内存是管理在连接对象里面的。这些资源会在连接断开的时候才释放。所以如果长连接累积下来，可能导致内存占用太大，被系统强行杀掉（OOM），从现象看就是 MySQL 异常重启了。\n\n怎么解决这个问题呢？你可以考虑以下两种方案。\n\n1.定期断开长连接。使用一段时间，或者程序里面判断执行过一个占用内存的大查询后，断开连接，之后要查询再重连。\n\n2.如果你用的是 MySQL 5.7 或更新版本，可以在每次执行一个比较大的操作后，通过执行 mysql_reset_connection 来重新初始化连接资源。这个过程不需要重连和重新做权限验证，但是会将连接恢复到刚刚创建完时的状态。\n\n**查询缓存**\n\n连接建立完成后，你就可以执行 select 语句了。执行逻辑就会来到第二步：查询缓存。\n\nMySQL 拿到一个查询请求后，会先到查询缓存看看，之前是不是执行过这条语句。之前执行过的语句及其结果可能会以 key-value 对的形式，被直接缓存在内存中。key 是查询的语句，value 是查询的结果。如果你的查询能够直接在这个缓存中找到 key，那么这个 value 就会被直接返回给客户端。\n\n如果语句不在查询缓存中，就会继续后面的执行阶段。执行完成后，执行结果会被存入查询缓存中。你可以看到，如果查询命中缓存，MySQL 不需要执行后面的复杂操作，就可以直接返回结果，这个效率会很高。\n\n**但是大多数情况下我会建议你不要使用查询缓存，为什么呢？因为查询缓存往往弊大于利。**\n\n查询缓存的失效非常频繁，只要有对一个表的更新，这个表上所有的查询缓存都会被清空。因此很可能你费劲地把结果存起来，还没使用呢，就被一个更新全清空了。对于更新压力大的数据库来说，查询缓存的命中率会非常低。除非你的业务就是有一张静态表，很长时间才会更新一次。比如，一个系统配置表，那这张表上的查询才适合使用查询缓存。\n\n好在 MySQL 也提供了这种“按需使用”的方式。你可以将参数 query_cache_type 设置成 DEMAND，这样对于默认的 SQL 语句都不使用查询缓存。而对于你确定要使用查询缓存的语句，可以用 SQL_CACHE 显式指定，像下面这个语句一样：\n\n```\nselect SQL_CACHE * from T where ID=10\n```\n\n需要注意的是，MySQL 8.0 版本直接将查询缓存的整块功能删掉了，也就是说 8.0 开始彻底没有这个功能了。\n\n**分析器**\n\n如果没有命中查询缓存，就要开始真正执行语句了。首先，MySQL 需要知道你要做什么，因此需要对 SQL 语句做解析。\n\n分析器先会做“词法分析”。你输入的是由多个字符串和空格组成的一条 SQL 语句，MySQL 需要识别出里面的字符串分别是什么，代表什么。\n\nMySQL 从你输入的\"select\"这个关键字识别出来，这是一个查询语句。它也要把字符串 “T”识别成“表名 T”，把字符串“ID”识别成“列 ID”。\n\n做完了这些识别以后，就要做“语法分析”。根据词法分析的结果，语法分析器会根据语法规则，判断你输入的这个 SQL 语句是否满足 MySQL 语法。\n\n如果你的语句不对，就会收到 “You have an error in your SQL syntax”的错误提醒，比如下面这个语句 select 少打了开头的字母“s”。\n\n```\nmysql> elect * from t where ID=1;\n```\n\n```\nERROR 1064 (42000): You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near 'elect * from t where ID=1' at line 1\n```\n\n一般语法错误会提示第一个出现错误的位置，所以你要关注的是紧接“use near”的内容。\n\n**优化器**\n\n经过了分析器，MySQL 就知道你要做什么了。在开始执行之前，还要先经过优化器的处理。\n\n优化器是在表里面有多个索引的时候，决定使用哪个索引；或者在一个语句有多表关联（join）的时候，决定各个表的连接顺序。比如你执行下面这样的语句，这个语句是执行两个表的 join:\n\n```\nselect * from t1 join t2 using(ID) where t1.c=10 and t2.d=20\n```\n\n- 既可以先从表 t1 里面取出 c=10 的记录的 ID 值，再根据 ID 值关联到表 t2，再判断 t2 里面 d 的值是否等于 20。\n- 也可以先从表 t2 里面取出 d=20 的记录的 ID 值，再根据 ID 值关联到 t1，再判断 t1 里面 c 的值是否等于 10。\n\n这两种执行方法的逻辑结果是一样的，但是执行的效率会有不同，而优化器的作用就是决定选择使用哪一个方案。\n\n优化器阶段完成后，这个语句的执行方案就确定下来了，然后进入执行器阶段。\n\n**执行器**\n\nMySQL 通过分析器知道了你要做什么，通过优化器知道了该怎么做，于是就进入了执行器阶段，开始执行语句。\n\n开始执行的时候，要先判断一下你对这个表 T 有没有执行查询的权限，如果没有，就会返回没有权限的错误，如下所示 (在工程实现上，如果命中查询缓存，会在查询缓存返回结果的时候，做权限验证。查询也会在优化器之前调用 precheck 验证权限)。\n\n```\nselect * from t1 where id=10;\nERROR 1142(42000) SELECT commend denied to user 'b'@'localhost' for table 'T'\n```\n\n如果有权限，就打开表继续执行。打开表的时候，执行器就会根据表的引擎定义，去使用这个引擎提供的接口\n\n比如我们这个例子中的表 T 中，ID 字段没有索引，那么执行器的执行流程是这样的：\n \n1.调用 InnoDB 引擎接口取这个表的第一行，判断 ID 值是不是 10，如果不是则跳过，如果是则将这行存在结果集中；\n2.调用引擎接口取“下一行”，重复相同的判断逻辑，直到取到这个表的最后一行。\n3.执行器将上述遍历过程中所有满足条件的行组成的记录集作为结果集作为结果集返回给客户端。\n\n至此，这个语句就执行完成了。\n\n对于有索引的表，执行的逻辑也差不多。第一次调用的是“取满足条件的第一行”这个接口，之后循环取“满足条件的下一行”这个接口，这些接口都是引擎中已经定义好的。\n\n你会在数据库的慢查询日志中看到一个 rows_examined 的字段，表示这个语句执行过程中扫描了多少行。这个值就是在执行器每次调用引擎获取数据行的时候累加的。\n\n在有些场景下，执行器调用一次，在引擎内部则扫描了多行，因此引擎扫描行数跟 rows_examined 并不是完全相的。\n\n\n\n\n\n\n\n\n\n\n\n\n","tags":["mysql"],"categories":["MYSQL"]}]